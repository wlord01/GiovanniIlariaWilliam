#!/usr/bin/env python

"""
Phase 2 data script

Contains functions for performance data acquisition and analysis of
phase 2 experiments.

Functions:
- test_trial() -- Tests performance of one trial
- test_multiple_trials() -- Tests performance of multiple trials and
  saves data to file
- check_completion() -- Check which trials completed the goal
- check_completion_ratio() -- Returns the number of successful trials
  and ratio
- check_completion_time() -- Returns the average number of steps
  required to complete the goal in an experiment for the system version
- plot_completion_times() -- Plots the completion times for the system
  versions in the different experiments
"""

import numpy as np
import matplotlib.pyplot as plt

import phase_2


def test_trial(model_type, trial_number, number_of_simulations):
    """Test performance of one trial

    Keyword arguments:
    - model_type (string) -- The model type (run in phase 1) to test
    - trial_number (string) -- The trial number (run in phase 1) to
      test
    - number_of_simulations (int) -- desired number of simulations

    Tests the goal based planner by running a desired number of
    simulations and returning data on goal accomplishment and
    number of required steps for goal based planning and data on reward
    for utility reasoning experiments. Set graphics_on to False in
    phase_2.py for much faster runtime.
    """
    trial_data = []
    for simulation_number in range(number_of_simulations):
        simulation_data = phase_2.main(model_type, trial_number)
        trial_data.append(simulation_data)

    return np.array(trial_data)


def test_multiple_trials(model_type, trials, number_of_simulations):
    """Test performance of multiple trials

    Keyword arguments:
    - model_type (string) -- The model type (run in phase 1) to test
    - trials (range of ints) -- The trials (run in phase 1) to test
    - number_of_simulations (int) -- desired number of simulations per
      trial

    Tests all the trials by running a desired number of simulations
    for each in the current scenario and returns data array in which
    first column is goal accomplished yes/no (1/0) and second
    column is number of required steps for completion in the case of
    goal based planning. For utility reasoning experiments, the data
    array contains data on acquired utility. Set graphics_on to False
    in phase_2.py for much faster runtime.
    """
    model_data = []
    for trial_number in trials:
        trial_data = test_trial(model_type, trial_number,
                                number_of_simulations
                                )
        model_data.append(trial_data)

    return np.array(model_data)


def check_completion_ratio(multiple_trials_data):
    """Check ratio of successful trials

    Keyword arguments:
    - multiple_trials_data -- data as generated by function
      test_multiple_trials()

    Returns the number of successful trials and ratio of successful
    trials.
    """
    number_of_trials = multiple_trials_data.shape[0]
    number_of_simulations = multiple_trials_data.shape[1]
    completion_data = multiple_trials_data[:, :, 0]
    complete_simulations = np.count_nonzero(completion_data, 1)
    condition = complete_simulations >= 0.9 * number_of_simulations
    complete_trials = len(np.where(condition)[0])
    completion_ratio = complete_trials / number_of_trials

    return (complete_trials, completion_ratio)


def get_successful_trials(multiple_trials_data):
    """Returns data array with successful trials

    Keyword arguments:
    - multiple_trials_data -- data as generated by function
      test_multiple_trials()

    Checks which of the trials completed the goal in more than 90%
    of the simulation runs. Returns data array of successful trials.
    """
    number_of_simulations = multiple_trials_data.shape[1]
    completion_data = multiple_trials_data[:, :, 0]
    complete_simulations = np.count_nonzero(completion_data, 1)
    condition = complete_simulations >= 0.9 * number_of_simulations
    complete_trials = np.where(condition)[0]

    return multiple_trials_data[complete_trials]


def check_completion_time(successful_trials_data):
    """Check average runtime of successful trials

    Keyword arguments:
    - successful_trials_data -- data as generated by function
      get_successful_trials()

    Calculates the number or required steps for accomplishing goal
    averaged over all successful simulations of system version. Returns
    average number of steps and standard deviation.
    """
    runtime_data = successful_trials_data[:, :, 1]
    trial_mean_runtimes = np.mean(runtime_data, 1)
    mean_runtime = np.mean(trial_mean_runtimes)
    runtime_sem = (np.std(trial_mean_runtimes) / np.sqrt(len(runtime_data)))

    return (trial_mean_runtimes, mean_runtime, runtime_sem)


def plot_completion_times(data_list):
    """Plot completion times

    Keyword arguments:
    data_list -- list of data arrays from different scenarios (set-ups
                 of environment and goal)
    line_style (string) -- Definition for line style in matplotlib
                           plotting standard

    Plots average completion times for different scenarios (set-ups of
    environment and goal) and fitted polynomial, defined within the
    function. Shows the dependency of completion time on number of
    sub-goals if the scenarios have increasing number of sub-goals.
    """
    from scipy.optimize import curve_fit

    def func(x, a, b):  # POLYNOMIAL CURVE-FIT
        return a*x**2 + b*x

    plt.figure()
    plt.xlabel('Scenario')
    plt.ylabel('Completion time (steps)')
    box_plot_data = []
    regression_data = []
    for experiment_data in data_list:
        successful_trials_data = get_successful_trials(experiment_data)
        (trial_mean_runtimes,
         mean_runtime,
         runtime_sem
         ) = check_completion_time(successful_trials_data)
        box_plot_data.append(trial_mean_runtimes)
        regression_data.append(np.median(trial_mean_runtimes))

    plt.boxplot(box_plot_data)

    # CURVE-FIT
    x = np.arange(1, len(data_list) + 1)
    popt, pcov = curve_fit(func, x, regression_data)
    xx = np.arange(0, len(data_list) + 2, 0.1)
    plt.xlim(0.5, 5.5)
    plt.plot(xx, func(xx, *popt), '--')

    return popt


def plot_utility_bars(data1, data2):
    """
    Plot bar chart for utility data

    Keyword arguments:
    - data1: array containing first data set
    - data2: array containing second data set

    The function plots the mean accumulated utility for different number of
    allowed action attempts, for the two models.
    """
#    data1_means = (5, 15, 30)  # , 40)
#    data2_means = (6, 13, 32)  # , 38)
#    data1_std = (2, 3, 4)  # , 5)
#    data2_std = (2, 3, 3)  # , 6)
    data1_means = np.mean(data1, axis=1)
    data2_means = np.mean(data2, axis=1)
    data1_error = np.std(data1, axis=1) / np.sqrt(len(data1[0]))
    data2_error = np.std(data2, axis=1) / np.sqrt(len(data2[0]))
    n = len(data1_means)
    ind = np.arange(n)    # the x locations for the groups
    w = 0.4
    labels = [str(i + 1) for i in range(n)]

    # Pull the formatting out here
    bar_kwargs = {'width': w, 'color': 'black', 'yerr': data1_error,
                  'ecolor': 'grey', 'capsize': 5, 'tick_label': labels
                  }

    fig, ax = plt.subplots()
    ax.bar(ind, data1_means, **bar_kwargs)

    bar_kwargs = {'width': w, 'color': 'white', 'yerr': data2_error,
                  'ecolor': 'black', 'capsize': 5, 'tick_label': labels
                  }
    ax.bar(ind + w, data2_means, **bar_kwargs)

    def plot_significance(i, text, data1_means, data2_means):
        # TEST OF P-VALUE ARROWS
        (y_bottom, y_top) = ax.get_ylim()
        y_height = y_top - y_bottom
        x = ind[i] + w / 2
        y = max(data1_means[i], data2_means[i]) + (y_height * 0.03)
        props = {'connectionstyle': 'bar', 'arrowstyle': '-', 'shrinkA': 6,
                 'shrinkB': 0
                 }

        ax.text(x + w / 2, y + (y_height * 0.02), text, ha='center',
                va='bottom', size='small'
                )
        ax.annotate('', xy=(x, y), xytext=(x + w, y), arrowprops=props)

    for i in range(n):
        (t, p) = st.ttest_ind(data1[i], data2[i], equal_var=False)
        if p <= 0.001:
            plot_significance(i, '***', data1_means, data2_means)
        elif p <= 0.01:
            plot_significance(i, '**', data1_means, data2_means)
        elif p <= 0.05:
            plot_significance(i, '*', data1_means, data2_means)
#        elif p <= 0.1:
#            plot_significance(i, '****', data1_means, data2_means)

    (y_bottom, y_top) = ax.get_ylim()
    y_height = y_top - y_bottom
    ax.text(0.2, y_height - 0.1*y_height,
            '*) p < 0.05, **) p < 0.01, ***) p < 0.001', ha='left',
            va='bottom', size='x-small'
            )

    plt.ylabel('Utility')
    plt.xlabel('Action attempts')

    # FOR CHECKED ACTIONS BAR CHART
#    props = {'arrowstyle': '-', 'connectionstyle': 'bar', 'shrinkA': 60,
#             'shrinkB': 0
#             }
#    ax.set_xlim(-0.05, 0.85)
#    ax.set_ylim(0, 4.5)
#    ax.set_ylabel('Checked actions')
#    ax.set_xticks((w / 2, 3*w / 2))
#    ax.set_xticklabels(('Restricted search', 'Nonrestricted search'))
#    ax.annotate('', xy=(0.2, 2.8), xytext=(0.6, 2.8), arrowprops=props)
#    (t, p) = st.stats.ttest_ind(data1[0], data2[0], equal_var=False)
#    ax.text(0.4, 3.8, 'p={}'.format(str(round(p, 8))), ha='center',
#            va='bottom', size='small'
#            )

    plt.tight_layout()

    plt.show()


if __name__ == '__main__':
    # TESTS
    import time
    import scipy.stats as st
    import matplotlib
    matplotlib.rcParams.update({'font.size': 18})
#    start = time.time()

#    scenario = 5
#    for model_type in ['IMPs']:
#        data = test_multiple_trials(model_type, range(1, 11), 100)
#        file_name = 'Data/10trial_ExtE{}_{}'.format(str(scenario),
#                                                    str(model_type)
#                                                    )
#        np.save(file_name, data)

#    data = test_multiple_trials('IGN', range(1, 5), 100)
#    (complete_trials, completion_ratio) = check_completion_ratio(data)
#    complete_trials_data = get_successful_trials(data)
#    (trial_mean_runtimes,
#     mean_runtime,
#     runtime_sem
#     ) = check_completion_time(complete_trials_data)
#    print((time.time() - start) / 60)
#    conf_int = st.t.interval(0.95, len(trial_mean_runtimes)-1,
#                             loc=np.mean(trial_mean_runtimes),
#                             scale=st.sem(trial_mean_runtimes)
#                             )

#    suffix = 'IMP'
#    data1 = np.load('Data/10trial_ExtE1_{}.npy'.format(suffix))
#    data2 = np.load('Data/10trial_ExtE2_{}.npy'.format(suffix))
#    data3 = np.load('Data/10trial_ExtE3_{}.npy'.format(suffix))
#    data4 = np.load('Data/10trial_ExtE4_{}.npy'.format(suffix))
#    data5 = np.load('Data/10trial_ExtE5_{}.npy'.format(suffix))
#    (complete_trials1, completion_ratio1) = check_completion_ratio(data1)
#    (complete_trials2, completion_ratio2) = check_completion_ratio(data2)
#    (complete_trials3, completion_ratio3) = check_completion_ratio(data3)
#    (complete_trials4, completion_ratio4) = check_completion_ratio(data4)
#    (complete_trials5, completion_ratio5) = check_completion_ratio(data5)

#    z = plot_completion_times([data1, data2, data3, data4, data5])

# UTILITY REASONING
utility_planner_data = np.load('Data/UtilityPlannerDataIMPs.npy')
simple_planner_data = np.load('Data/SimplePlannerDataIMPs.npy')
plot_utility_bars(simple_planner_data, utility_planner_data)
#plot_utility_bars(np.array([restricted_search_data]),
#                  np.array([nonrestricted_search_data])
#                  )
